{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Layer Perceptron (MLP) for XOR Function\n",
    "\n",
    "Objective:\n",
    "The aim of this experiment is to implement a Multi-Layer Perceptron (MLP) with one hidden layer using NumPy and demonstrate that it can learn the XOR Boolean function, which is not linearly separable.\n",
    "\n",
    "Background:\n",
    "A single-layer perceptron fails to learn the XOR function because XOR is not linearly separable. However, an MLP with at least one hidden layer can solve this problem by learning a non-linear decision boundary.\n",
    "\n",
    "Implementation Details:\n",
    "Activation Function:\n",
    "\n",
    "The sigmoid function is used for activation in both hidden and output layers since it maps inputs to a (0,1) range and allows for non-linearity.\n",
    "The derivative of the sigmoid function is used during backpropagation for weight updates.\n",
    "Network Architecture:\n",
    "\n",
    "Input Layer: 2 neurons (for the two inputs of XOR).\n",
    "Hidden Layer: 2 neurons (configurable).\n",
    "Output Layer: 1 neuron (to classify the XOR output).\n",
    "Training the MLP:\n",
    "\n",
    "The network initializes weights and biases randomly.\n",
    "It performs forward propagation to compute outputs.\n",
    "The error between predicted and actual outputs is computed.\n",
    "Using backpropagation, the weights and biases are updated using the gradient descent rule.\n",
    "This process is repeated for a specified number of epochs (10,000).\n",
    "Prediction Process:\n",
    "\n",
    "Once trained, the model takes input values, passes them through the hidden layer and output layer, and generates the final predictions.\n",
    "If the output is greater than 0.5, it is classified as 1, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Predictions: [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Importing numpy for numerical operations\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Function to train a Multi-Layer Perceptron (MLP) with one hidden layer\n",
    "def mlp_train(X, y, hidden_neurons=2, lr=0.1, epochs=10000):\n",
    "    np.random.seed(42)  # Setting seed for reproducibility\n",
    "    input_neurons = X.shape[1]  # Number of input neurons\n",
    "    output_neurons = 1  # Single output neuron\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    weights_input_hidden = np.random.rand(input_neurons, hidden_neurons)\n",
    "    bias_hidden = np.random.rand(hidden_neurons)\n",
    "    weights_hidden_output = np.random.rand(hidden_neurons, output_neurons)\n",
    "    bias_output = np.random.rand(output_neurons)\n",
    "    \n",
    "    # Training process\n",
    "    for _ in range(epochs):\n",
    "        # Forward propagation\n",
    "        hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "        hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "        output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "        output = sigmoid(output_layer_input)\n",
    "        \n",
    "        # Compute error\n",
    "        error = y.reshape(-1, 1) - output\n",
    "        \n",
    "        # Backpropagation\n",
    "        d_output = error * sigmoid_derivative(output)\n",
    "        d_hidden = d_output.dot(weights_hidden_output.T) * sigmoid_derivative(hidden_layer_output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        weights_hidden_output += hidden_layer_output.T.dot(d_output) * lr\n",
    "        bias_output += np.sum(d_output, axis=0) * lr\n",
    "        weights_input_hidden += X.T.dot(d_hidden) * lr\n",
    "        bias_hidden += np.sum(d_hidden, axis=0) * lr\n",
    "    \n",
    "    return weights_input_hidden, bias_hidden, weights_hidden_output, bias_output\n",
    "\n",
    "# Function to make predictions using trained MLP\n",
    "def mlp_predict(X, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output):\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "    return (sigmoid(output_layer_input) > 0.5).astype(int).flatten()\n",
    "\n",
    "# XOR Truth Table (Inputs and expected outputs)\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs for XOR gate\n",
    "y_xor = np.array([0, 1, 1, 0])  # Expected outputs for XOR gate\n",
    "\n",
    "# Train MLP for XOR gate\n",
    "weights_input_hidden, bias_hidden, weights_hidden_output, bias_output = mlp_train(X_xor, y_xor)\n",
    "y_pred_xor = mlp_predict(X_xor, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)\n",
    "\n",
    "# Printing the predictions\n",
    "print(\"XOR Predictions:\", y_pred_xor)  # Expected: [0, 1, 1, 0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
